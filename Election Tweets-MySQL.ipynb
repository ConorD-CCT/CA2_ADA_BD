{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c4ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, to_date\n",
    "from pyspark.sql.types import FloatType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cfed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd \n",
    "from pyspark.sql import SparkSession \n",
    "#appName = \"PySpark MySQL Example - via mysql.connector\" \n",
    "#master = \"local\" \n",
    "#spark=SparkSession.builder.master(master).appName(appName).getOrCreate() \n",
    "# Establish a connection \n",
    "conn=mysql.connector.connect(user='root', database='TrumpDB', password='password', host=\"localhost\", port=3306) \n",
    "cursor = conn.cursor() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bbc70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3762/4065591516.py:4: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pdf = pd.read_sql(query, con=conn)\n"
     ]
    }
   ],
   "source": [
    "#query = \"SELECT * FROM trumptable\" \n",
    "query = \"SELECT * FROM trumptable WHERE candidate_id = 2\" \n",
    "# Create a pandas dataframe \n",
    "pdf = pd.read_sql(query, con=conn) \n",
    "conn.close() \n",
    "# Convert Pandas dataframe to spark DataFrame \n",
    "df = spark.createDataFrame(pdf) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30aaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c003be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b24c3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                         (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o42.showString",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7605/3774136243.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mjdbcDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o42.showString"
     ]
    }
   ],
   "source": [
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/TrumpDB\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"trumptable\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b96987",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '2016_US_election_tweets.csv'\n",
    "\n",
    "# Read input file from hadoop directory and convert to pandas\n",
    "df = spark.read.csv('/user1/CA2/'+filename,header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c696a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef35ac0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.select('candidate_id', \"created_at\", \"tweet_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96557282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_nulls = df.na.drop()\n",
    "tweets = df_no_nulls.filter(df.candidate_id == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012695a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    return analyzer.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "sentiment_udf = udf(analyze_sentiment, FloatType())\n",
    "spark.udf.register(\"sentiment_udf\", sentiment_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to date\n",
    "tweets = tweets.withColumn(\"date\", to_date(col(\"created_at\")))\n",
    "\n",
    "# Calculate sentiment score for each tweet\n",
    "tweets = tweets.withColumn(\"sentiment\", sentiment_udf(col(\"tweet_text\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361650ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.select('tweet_text','date','sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be125f9c",
   "metadata": {},
   "source": [
    "tweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c84466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "# filter out out-of-range dates\n",
    "tweets = tweets.filter((year(\"date\") >= 1) & (year(\"date\") <= 9999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment_df = tweets.groupBy(\"date\") \\\n",
    "    .agg(F.mean(\"sentiment\").alias(\"avg_sentiment\")) \\\n",
    "    .orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a035f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea265bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "daily_sentiment_pd = daily_sentiment_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = daily_sentiment_pd.iloc[:194,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc32e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['date'] = pd.to_datetime(df2['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea6d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df2['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[(df2['date'].dt.year >= 2016) & (df2['date'].dt.year <= 2017)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d575178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639fb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df2['date'],df2['avg_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02156c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a449ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x = df2.date,y = df2.avg_sentiment)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67eed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(x) for x in df2['date']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a8583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
